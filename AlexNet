{"metadata":{"kernelspec":{"display_name":"Python (tf_gpu)","language":"python","name":"tf_gpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"19700b33-8328-4a0a-9b6b-7aa97c61027e","cell_type":"code","source":"#Part-55:\n\nimport os\nimport time\nimport math\nimport random\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    roc_curve, auc\n)\nfrom sklearn.preprocessing import label_binarize\n\n\nDATASET_PATH = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/art_classification_model/dataset\"\nTRAIN_DIR = os.path.join(DATASET_PATH, \"train\")\nVAL_DIR   = os.path.join(DATASET_PATH, \"val\")\nTEST_DIR  = os.path.join(DATASET_PATH, \"test\")\n\nSAVE_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model\"\nos.makedirs(SAVE_DIR, exist_ok=True)\nBEST_CKPT_PATH = os.path.join(SAVE_DIR, \"best_model_state_dict.pt\")  \nSCRIPTED_MODEL_PATH = os.path.join(SAVE_DIR, \"model_script.pt\")\nTRACED_MODEL_PATH   = os.path.join(SAVE_DIR, \"model_trace.pt\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nUSE_AMP = torch.cuda.is_available()\nprint(f\"Using device: {device} | AMP: {USE_AMP}\")\n\ndef set_seed(s=42):\n    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(s)\nset_seed(42)\n\n\ntorch.backends.cudnn.benchmark = True\n\n\nBATCH_SIZE = 64\nNUM_WORKERS = 8\nNUM_EPOCHS = 30\n\n\nBACKBONE = \"resnet50\"   \nSTAGED_UNFREEZE = True   \nUNFREEZE_AT_EPOCH = 5    \n\n\nBASE_LR_HEAD = 3e-4      \nBASE_LR_FEAT = 1e-5      \nWEIGHT_DECAY = 1e-4\nWARMUP_EPOCHS = 3\n\n\nLABEL_SMOOTH = 0.05\nUSE_CLASS_WEIGHTS = False\nUSE_WEIGHTED_SAMPLER = False\n\n\nEARLY_STOPPING_MODE = \"downtrend\"  \nEARLY_STOPPING_PATIENCE = 5\nDOWNTREND_EPS = 1e-4","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda | AMP: True\n"]}],"execution_count":1},{"id":"e74f9585-9eb3-479c-a909-12fbc5b8a988","cell_type":"code","source":"#Part-56:\n\nimport os, time\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchvision import datasets\n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\ntrain_plain = datasets.ImageFolder(root=TRAIN_DIR)\nclass_names = train_plain.classes\nprint(f\"Classes ({len(class_names)}): {class_names}\")\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n\ndef count_images_per_class(root_dir, classes):\n    counts = []\n    for c in classes:\n        class_dir = os.path.join(root_dir, c)\n        if not os.path.isdir(class_dir):\n            counts.append(0)\n            continue\n        files = []\n        for f in os.listdir(class_dir):\n            fp = os.path.join(class_dir, f)\n            if os.path.isfile(fp) and os.path.splitext(f.lower())[1] in IMG_EXTS:\n                files.append(f)\n        counts.append(len(files))\n    return counts\n\ntrain_counts = count_images_per_class(TRAIN_DIR, class_names)\nval_counts   = count_images_per_class(VAL_DIR,   class_names)\ntest_counts  = count_images_per_class(TEST_DIR,  class_names)\n\nfig, axes = plt.subplots(4, 1, figsize=(16, 30))\n\ndef style_x(ax):\n    ax.set_xticks(range(len(class_names)))\n    ax.set_xticklabels(class_names, rotation=90, fontsize=10)\n    ax.set_xlabel(\"Class Name\", fontsize=14)\n    ax.set_ylabel(\"Number of Images\", fontsize=14)\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n\naxes[0].bar(range(len(class_names)), train_counts)\naxes[0].set_title('Training Image Count per Class', fontsize=16)\nstyle_x(axes[0])\n\naxes[1].bar(range(len(class_names)), val_counts)\naxes[1].set_title('Validation Image Count per Class', fontsize=16)\nstyle_x(axes[1])\n\naxes[2].bar(range(len(class_names)), test_counts)\naxes[2].set_title('Test Image Count per Class', fontsize=16)\nstyle_x(axes[2])\n\nbar_width = 0.25\nx = list(range(len(class_names)))\naxes[3].bar([i - bar_width for i in x], train_counts, width=bar_width, label='Train')\naxes[3].bar(x, val_counts, width=bar_width, label='Validation')\naxes[3].bar([i + bar_width for i in x], test_counts, width=bar_width, label='Test')\naxes[3].set_title('Combined Count per Class (Train/Val/Test)', fontsize=16)\naxes[3].set_xticks(x)\naxes[3].set_xticklabels(class_names, rotation=90, fontsize=10)\naxes[3].set_xlabel(\"Class Name\", fontsize=14)\naxes[3].set_ylabel(\"Number of Images\", fontsize=14)\naxes[3].legend()\naxes[3].grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout(pad=4)\n\nts = time.strftime(\"%Y%m%d-%H%M%S\")\nimg_path = os.path.join(SAVE_OUTPUT_DIR, f\"class_counts_{ts}.jpg\")\ncsv_path = os.path.join(SAVE_OUTPUT_DIR, f\"class_counts_{ts}.csv\")\n\nfig.savefig(img_path, dpi=200, bbox_inches=\"tight\"); plt.close(fig)\n\npd.DataFrame({\n    \"class\": class_names,\n    \"train\": train_counts,\n    \"val\":   val_counts,\n    \"test\":  test_counts\n}).to_csv(csv_path, index=False)\n\nprint(f\"[Saved] Figure → {img_path}\")\nprint(f\"[Saved] Counts CSV → {csv_path}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Classes (21): ['abstract_art', 'abstract_expressionism', 'amateur', 'art_nouveau', 'baroque', 'chinese_landscape', 'constructivism', 'cubism', 'expressionism', 'fauvism', 'futurism', 'high_renaissance', 'minimalism', 'op_art', 'pop_art', 'post_impressionism', 'realism', 'renaissance', 'romanticism', 'surrealism', 'symbolism']\n","[Saved] Figure → /home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet/class_counts_20250907-082934.jpg\n","[Saved] Counts CSV → /home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet/class_counts_20250907-082934.csv\n"]}],"execution_count":2},{"id":"4ab04264-4987-48e1-b50f-9ae8f5bbac87","cell_type":"code","source":"#Part-57:\n\nfrom torchvision import datasets, transforms, models\n\n\ntrain_tf = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(0.2, 0.2, 0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\neval_tf = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\ntrain_data = datasets.ImageFolder(root=TRAIN_DIR, transform=train_tf)\nval_data   = datasets.ImageFolder(root=VAL_DIR,   transform=eval_tf)\ntest_data  = datasets.ImageFolder(root=TEST_DIR,  transform=eval_tf)\n\nnum_classes = len(train_data.classes)\nprint(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)} | Classes: {num_classes}\")\n\nsampler = None\ncriterion_weights = None\n\nif USE_WEIGHTED_SAMPLER:\n    train_targets = [y for _, y in train_data.samples]\n    cnt = Counter(train_targets)\n    weights_per_sample = [1.0 / cnt[y] for y in train_targets]\n    sampler = WeightedRandomSampler(weights_per_sample, num_samples=len(weights_per_sample), replacement=True)\n\nif USE_CLASS_WEIGHTS and not USE_WEIGHTED_SAMPLER:\n    train_targets = [y for _, y in train_data.samples]\n    cnt = Counter(train_targets)\n    class_w = torch.tensor([1.0 / cnt[i] for i in range(num_classes)], dtype=torch.float32, device=device)\n    criterion_weights = class_w\n\nloader_args = dict(\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    persistent_workers=(NUM_WORKERS > 0),\n)\n\nif sampler is not None:\n    train_loader = DataLoader(train_data, sampler=sampler, **loader_args)\nelse:\n    train_loader = DataLoader(train_data, shuffle=True, **loader_args)\n\nval_loader  = DataLoader(val_data, shuffle=False, **loader_args)\ntest_loader = DataLoader(test_data, shuffle=False, **loader_args)\n\n\nclass ModelWrapper(nn.Module):\n    def __init__(self, backbone_name: str, num_classes: int):\n        super().__init__()\n        self.backbone_name = backbone_name.lower()\n        if self.backbone_name == \"alexnet\":\n            from torchvision.models import alexnet, AlexNet_Weights\n            try:\n                net = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n                print(\"Loaded torchvision AlexNet pretrained weights.\")\n            except Exception as e:\n                print(f\"AlexNet pretrained load failed ({e}), using random init.\")\n                net = models.alexnet(weights=None)\n          \n            for p in net.features.parameters():\n                p.requires_grad = False\n            in_features = net.classifier[6].in_features  \n            new_head = list(net.classifier)\n            new_head[6] = nn.Sequential(\n                nn.Linear(in_features, 1024),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.3),\n                nn.Linear(1024, num_classes)\n            )\n            net.classifier = nn.Sequential(*new_head)\n            self.model = net\n\n        elif self.backbone_name == \"resnet50\":\n            from torchvision.models import resnet50, ResNet50_Weights\n            net = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n            in_features = net.fc.in_features\n            net.fc = nn.Linear(in_features, num_classes)\n          \n            if STAGED_UNFREEZE:\n                for name, p in net.named_parameters():\n                    if not name.startswith(\"fc.\"):  \n                        p.requires_grad = False\n            self.model = net\n        else:\n            raise ValueError(\"Unsupported BACKBONE. Use 'alexnet' or 'resnet50'.\")\n\n    def forward(self, x):\n        return self.model(x)\n\nmodel = ModelWrapper(BACKBONE, num_classes).to(device)\n\n\nmodel = model.to(memory_format=torch.channels_last)\n\n\ntry:\n    model = torch.compile(model)\n    print(\"Model compiled with torch.compile.\")\nexcept Exception as e:\n    print(f\"torch.compile skipped: {e}\")\n\n\ncriterion = nn.CrossEntropyLoss(weight=criterion_weights, label_smoothing=LABEL_SMOOTH)\n\n\nhead_params, feat_params = [], []\nfor n, p in model.named_parameters():\n    if not p.requires_grad:\n        continue\n  \n    if (\"fc\" in n) or (\"classifier\" in n):\n        head_params.append(p)\n    else:\n        feat_params.append(p)\n\noptimizer = torch.optim.AdamW(\n    [\n        {\"params\": feat_params, \"lr\": BASE_LR_FEAT},\n        {\"params\": head_params, \"lr\": BASE_LR_HEAD},\n    ],\n    weight_decay=WEIGHT_DECAY\n)\n\n\ndef lr_lambda(epoch):\n    if epoch < WARMUP_EPOCHS:\n        return float(epoch + 1) / max(1, WARMUP_EPOCHS)\n    t = (epoch - WARMUP_EPOCHS) / max(1, (NUM_EPOCHS - WARMUP_EPOCHS))\n    return 0.5 * (1.0 + math.cos(math.pi * t))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\nscaler = GradScaler(enabled=USE_AMP)\n\nprint(f\"Setup complete ({BACKBONE}). Staged unfreeze: {STAGED_UNFREEZE}, unfreeze at epoch {UNFREEZE_AT_EPOCH}.\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 5168 | Val: 512 | Test: 511 | Classes: 21\n","Model compiled with torch.compile.\n","Setup complete (resnet50). Staged unfreeze: True, unfreeze at epoch 5.\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_89768/1140185888.py:145: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler(enabled=USE_AMP)\n"]}],"execution_count":3},{"id":"090e2d1b-80b5-4849-b606-695cc6290314","cell_type":"code","source":"#Part-58:\n\nimport time\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom torch import amp  \n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\n\nSHOW_PLOTS = True \n\n\nepoch_times = []\n\ndef run_one_epoch_train(model, loader, optimizer, scaler, criterion, device):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in loader:\n        images = images.to(device, non_blocking=True, memory_format=torch.channels_last)\n        labels = labels.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        # UPDATED autocast API\n        with amp.autocast(\"cuda\", enabled=USE_AMP):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    return running_loss / len(loader), correct / total\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in loader:\n        images = images.to(device, non_blocking=True, memory_format=torch.channels_last)\n        labels = labels.to(device, non_blocking=True)\n        # UPDATED autocast API\n        with amp.autocast(\"cuda\", enabled=USE_AMP):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        running_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    return running_loss / len(loader), correct / total\n\ndef _is_strict_increase(seq, eps=0.0):\n    return all((seq[i] - seq[i-1]) > eps for i in range(1, len(seq)))\n\ndef _is_strict_decrease(seq, eps=0.0):\n    return all((seq[i-1] - seq[i]) > eps for i in range(1, len(seq)))\n\ndef _maybe_unfreeze_features(epoch: int):\n    \"\"\"Unfreeze feature extractor and rebuild optimizer param groups once.\"\"\"\n    if not STAGED_UNFREEZE: \n        return False\n    if epoch != UNFREEZE_AT_EPOCH:\n        return False\n\n    unfrozen = 0\n    for n, p in model.named_parameters():\n        if (\"fc\" in n) or (\"classifier\" in n):\n            continue\n        if not p.requires_grad:\n            p.requires_grad = True\n            unfrozen += 1\n\n    if unfrozen > 0:\n        print(f\"[Unfreeze] Enabled gradient for {unfrozen} feature parameters at epoch {epoch}. Rebuilding optimizer groups...\")\n        head_params, feat_params = [], []\n        for n, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            if (\"fc\" in n) or (\"classifier\" in n):\n                head_params.append(p)\n            else:\n                feat_params.append(p)\n       \n        global optimizer, scheduler\n        optimizer = torch.optim.AdamW(\n            [\n                {\"params\": feat_params, \"lr\": BASE_LR_FEAT},\n                {\"params\": head_params, \"lr\": BASE_LR_HEAD},\n            ],\n            weight_decay=WEIGHT_DECAY\n        )\n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n        return True\n    return False\n\nbest_val_loss = float(\"inf\")\nbest_val_acc = -float(\"inf\")\nepochs_no_improve = 0\n\nval_loss_window = deque(maxlen=EARLY_STOPPING_PATIENCE)\nval_acc_window  = deque(maxlen=EARLY_STOPPING_PATIENCE)\n\nhistory = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"lr\": [], \"epoch_time\": []}\n\nprint(\"\\nStarting training...\")\nfor epoch in range(1, NUM_EPOCHS + 1):\n    \n    _ = _maybe_unfreeze_features(epoch)\n\n    t0 = time.time()\n\n    train_loss, train_acc = run_one_epoch_train(model, train_loader, optimizer, scaler, criterion, device)\n    val_loss, val_acc     = evaluate(model, val_loader, criterion, device)\n\n    scheduler.step()\n    lr_now = scheduler.get_last_lr()[0]  \n\n    \n    epoch_time = time.time() - t0\n    epoch_times.append(epoch_time)\n\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n    history[\"lr\"].append(lr_now)\n    history[\"epoch_time\"].append(epoch_time)\n\n    dt = time.time() - t0\n    print(f\"Epoch {epoch:02d}/{NUM_EPOCHS} | \"\n          f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n          f\"Val Loss {val_loss:.4f} Acc {val_acc*100:.2f}% | \"\n          f\"LR {lr_now:.2e} | {dt:.1f}s\")\n\n    improved = val_loss < best_val_loss - 1e-12\n    if improved:\n        best_val_loss = val_loss\n        best_val_acc = max(best_val_acc, val_acc)\n        epochs_no_improve = 0\n        torch.save({\n            \"model\": model.state_dict(),\n            \"classes\": list(train_data.classes),\n            \"backbone\": BACKBONE,\n            \"model_args\": {\"num_classes\": num_classes},\n        }, BEST_CKPT_PATH)\n        print(f\"  → Saved best checkpoint to: {BEST_CKPT_PATH}\")\n    else:\n        epochs_no_improve += 1\n\n    stop_now = False\n    if EARLY_STOPPING_MODE == \"off\":\n        stop_now = False\n    elif EARLY_STOPPING_MODE == \"plateau\":\n        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n            stop_now = True\n    elif EARLY_STOPPING_MODE == \"downtrend\":\n        val_loss_window.append(val_loss)\n        val_acc_window.append(val_acc)\n        if len(val_loss_window) == EARLY_STOPPING_PATIENCE:\n            loss_up  = _is_strict_increase(list(val_loss_window), eps=DOWNTREND_EPS)\n            acc_down = _is_strict_decrease(list(val_acc_window),  eps=DOWNTREND_EPS)\n            if loss_up or acc_down:\n                stop_now = True\n\n    if stop_now:\n        print(f\"Early stopping triggered after epoch {epoch} (mode={EARLY_STOPPING_MODE}).\")\n        break\n\n\nts = time.strftime(\"%Y%m%d-%H%M%S\")\n\n\nfig1 = plt.figure(figsize=(8,6))\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Val Loss\")\nplt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\nplt.grid(True); plt.legend(); plt.tight_layout()\nloss_path = os.path.join(SAVE_OUTPUT_DIR, f\"loss_curve_{ts}.jpg\")\nfig1.savefig(loss_path, dpi=200, bbox_inches=\"tight\")\nif SHOW_PLOTS: plt.show()\nelse: plt.close(fig1)\nprint(f\"[Saved] Loss curve → {loss_path}\")\n\n\nfig2 = plt.figure(figsize=(8,6))\nplt.plot(history[\"train_acc\"], label=\"Train Acc\")\nplt.plot(history[\"val_acc\"], label=\"Val Acc\")\nplt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\nplt.grid(True); plt.legend(); plt.tight_layout()\nacc_path = os.path.join(SAVE_OUTPUT_DIR, f\"accuracy_curve_{ts}.jpg\")\nfig2.savefig(acc_path, dpi=200, bbox_inches=\"tight\")\nif SHOW_PLOTS: plt.show()\nelse: plt.close(fig2)\nprint(f\"[Saved] Accuracy curve → {acc_path}\")\n\n\nfig3 = plt.figure(figsize=(8,6))\nplt.plot(history[\"lr\"], label=\"LR\")\nplt.title(\"Learning Rate (Warmup + Cosine)\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"LR\")\nplt.grid(True); plt.legend(); plt.tight_layout()\nlr_path = os.path.join(SAVE_OUTPUT_DIR, f\"lr_curve_{ts}.jpg\")\nfig3.savefig(lr_path, dpi=200, bbox_inches=\"tight\")\nif SHOW_PLOTS: plt.show()\nelse: plt.close(fig3)\nprint(f\"[Saved] LR curve → {lr_path}\")\n\n\nfig4 = plt.figure(figsize=(8,6))\nplt.plot(history[\"epoch_time\"], label=\"Epoch Time (Seconds)\")\nplt.title(\"Training Time per Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Time (s)\")\nplt.grid(True); plt.legend(); plt.tight_layout()\ntime_path = os.path.join(SAVE_OUTPUT_DIR, f\"epoch_time_curve_{ts}.jpg\")\nfig4.savefig(time_path, dpi=200, bbox_inches=\"tight\")\nif SHOW_PLOTS: plt.show()\nelse: plt.close(fig4)\nprint(f\"[Saved] Epoch Time curve → {time_path}\")\n\n\nhist_df = pd.DataFrame(history)\ncsv_path = os.path.join(SAVE_OUTPUT_DIR, f\"training_history_{ts}.csv\")\nhist_df.to_csv(csv_path, index_label=\"epoch\")\nprint(f\"[Saved] Training history CSV → {csv_path}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting training...\n","Epoch 01/30 | Train Loss 2.9527 Acc 13.47% | Val Loss 2.8554 Acc 22.27% | LR 6.67e-06 | 10.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 02/30 | Train Loss 2.6778 Acc 34.58% | Val Loss 2.5810 Acc 34.57% | LR 1.00e-05 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 03/30 | Train Loss 2.3782 Acc 45.57% | Val Loss 2.3389 Acc 37.50% | LR 1.00e-05 | 4.3s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 04/30 | Train Loss 2.1343 Acc 51.92% | Val Loss 2.2044 Acc 39.45% | LR 9.97e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","[Unfreeze] Enabled gradient for 159 feature parameters at epoch 5. Rebuilding optimizer groups...\n","Epoch 05/30 | Train Loss 2.0133 Acc 53.46% | Val Loss 2.1666 Acc 40.43% | LR 6.67e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 06/30 | Train Loss 1.9383 Acc 55.90% | Val Loss 2.1079 Acc 41.02% | LR 1.00e-05 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 07/30 | Train Loss 1.8435 Acc 57.57% | Val Loss 2.0386 Acc 41.60% | LR 1.00e-05 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 08/30 | Train Loss 1.7592 Acc 57.72% | Val Loss 1.9951 Acc 45.12% | LR 9.97e-06 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 09/30 | Train Loss 1.6896 Acc 59.71% | Val Loss 1.9658 Acc 43.75% | LR 9.87e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 10/30 | Train Loss 1.6282 Acc 60.39% | Val Loss 1.9366 Acc 44.92% | LR 9.70e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 11/30 | Train Loss 1.5836 Acc 62.29% | Val Loss 1.9287 Acc 44.14% | LR 9.47e-06 | 4.3s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 12/30 | Train Loss 1.5410 Acc 63.12% | Val Loss 1.8984 Acc 46.68% | LR 9.18e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 13/30 | Train Loss 1.5009 Acc 65.02% | Val Loss 1.8925 Acc 45.31% | LR 8.83e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 14/30 | Train Loss 1.4670 Acc 65.63% | Val Loss 1.8659 Acc 47.07% | LR 8.43e-06 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 15/30 | Train Loss 1.4461 Acc 66.62% | Val Loss 1.8685 Acc 46.88% | LR 7.99e-06 | 4.1s\n","Epoch 16/30 | Train Loss 1.4193 Acc 66.85% | Val Loss 1.8568 Acc 46.48% | LR 7.50e-06 | 4.3s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 17/30 | Train Loss 1.3888 Acc 67.94% | Val Loss 1.8389 Acc 46.88% | LR 6.98e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 18/30 | Train Loss 1.3799 Acc 68.25% | Val Loss 1.8397 Acc 46.48% | LR 6.43e-06 | 4.1s\n","Epoch 19/30 | Train Loss 1.3663 Acc 69.18% | Val Loss 1.8335 Acc 47.85% | LR 5.87e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 20/30 | Train Loss 1.3504 Acc 69.56% | Val Loss 1.8342 Acc 46.68% | LR 5.29e-06 | 4.2s\n","Epoch 21/30 | Train Loss 1.3495 Acc 68.92% | Val Loss 1.8245 Acc 47.66% | LR 4.71e-06 | 4.2s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 22/30 | Train Loss 1.3241 Acc 70.49% | Val Loss 1.8242 Acc 47.66% | LR 4.13e-06 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 23/30 | Train Loss 1.3205 Acc 70.47% | Val Loss 1.8204 Acc 48.24% | LR 3.57e-06 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n","Epoch 24/30 | Train Loss 1.3044 Acc 70.84% | Val Loss 1.8155 Acc 47.85% | LR 3.02e-06 | 4.1s\n","  → Saved best checkpoint to: /home/dhrubo/Desktop/MerakiNexus-V2-main/save best model (state_dict) and scripted model/best_model_state_dict.pt\n"]}],"execution_count":null},{"id":"58f781ab-b4ed-4bb4-92de-a29f94979f5e","cell_type":"code","source":"#Part-59:\n\nimport os, time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import amp  \nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    roc_curve, auc\n)\nfrom sklearn.preprocessing import label_binarize\n\n\nSHOW_PLOTS = True  \n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\n\nckpt = torch.load(BEST_CKPT_PATH, map_location=device)\nmodel.load_state_dict(ckpt[\"model\"])\nmodel.eval()\n\n\nexport_model = getattr(model, \"_orig_mod\", model)\nexport_model.eval()\n\nclass_names = ckpt.get(\"classes\", train_data.classes)\n\n\ndef _try_torchscript_export(mod, scripted_path, traced_path):\n    ok = False\n\n    try:\n        import torch._dynamo as dynamo\n    except Exception:\n        dynamo = None\n\n    try:\n        if dynamo is not None:\n            with dynamo.disable():\n                scripted = torch.jit.script(mod)\n        else:\n            scripted = torch.jit.script(mod)\n        torch.jit.save(scripted, scripted_path)\n        print(f\"Scripted model saved at: {scripted_path}\")\n        ok = True\n        return ok\n    except Exception as e:\n        print(f\"[WARN] torch.jit.script failed ({e}); falling back to trace.\")\n\n   \n    try:\n        example = torch.randn(1, 3, 224, 224, device=device).to(memory_format=torch.channels_last)\n        if dynamo is not None:\n            with dynamo.disable():\n                traced = torch.jit.trace(mod, example)\n        else:\n            traced = torch.jit.trace(mod, example)\n        torch.jit.save(traced, traced_path)\n        print(f\"Traced model saved at: {traced_path}\")\n        ok = True\n    except Exception as e2:\n        print(f\"[ERROR] torch.jit.trace failed as well: {e2}\")\n        print(\"Skipping TorchScript export.\")\n    return ok\n\n_ = _try_torchscript_export(\n    export_model,\n    SCRIPTED_MODEL_PATH,\n    TRACED_MODEL_PATH\n)\n\n\nall_preds, all_labels, all_probs = [], [], []\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device, non_blocking=True, memory_format=torch.channels_last)\n        with amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n            outputs = model(images)\n            probs = F.softmax(outputs, dim=1).cpu().numpy()\n        preds = probs.argmax(axis=1)\n\n        all_probs.extend(probs)\n        all_preds.extend(preds)\n        all_labels.extend(labels.numpy())\n\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)\n\ntest_acc = accuracy_score(all_labels, all_preds)\nprint(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n\nreport = classification_report(\n    all_labels, all_preds, target_names=class_names,\n    digits=3, output_dict=True\n)\ndf_report = pd.DataFrame(report).transpose()\nprint(\"\\nPer-class Precision / Recall / F1:\\n\")\nprint(df_report[['precision', 'recall', 'f1-score']].round(3))\n\nts = time.strftime(\"%Y%m%d-%H%M%S\")\nreport_csv = os.path.join(SAVE_OUTPUT_DIR, f\"classification_report_{ts}.csv\")\nsummary_txt = os.path.join(SAVE_OUTPUT_DIR, f\"test_summary_{ts}.txt\")\npreds_csv   = os.path.join(SAVE_OUTPUT_DIR, f\"test_predictions_{ts}.csv\")\ncm_img_path = os.path.join(SAVE_OUTPUT_DIR, f\"confusion_matrix_{ts}.jpg\")\nroc_img_path= os.path.join(SAVE_OUTPUT_DIR, f\"roc_multiclass_{ts}.jpg\")\n\ndf_report.to_csv(report_csv)\nwith open(summary_txt, \"w\") as f:\n    f.write(f\"Test Accuracy: {test_acc:.6f}\\n\")\n    f.write(f\"Num classes: {len(class_names)}\\n\")\n    f.write(f\"Num test samples: {len(all_labels)}\\n\")\nprint(f\"[Saved] Classification report → {report_csv}\")\nprint(f\"[Saved] Summary → {summary_txt}\")\n\ntop1_prob = all_probs.max(axis=1)\npred_rows = pd.DataFrame({\n    \"true_label_idx\": all_labels,\n    \"true_label\": [class_names[i] for i in all_labels],\n    \"pred_label_idx\": all_preds,\n    \"pred_label\": [class_names[i] for i in all_preds],\n    \"correct\": (all_preds == all_labels).astype(int),\n    \"top1_prob\": top1_prob\n})\npred_rows.to_csv(preds_csv, index=False)\nprint(f\"[Saved] Test predictions → {preds_csv}\")\n\n\ncm = confusion_matrix(all_labels, all_preds)\nfig_cm = plt.figure(figsize=(max(8, 1.2*len(class_names)), max(6, 1.0*len(class_names))))\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title(f\"Confusion Matrix ({len(class_names)} Classes)\")\nplt.colorbar()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names, rotation=90, fontsize=9)\nplt.yticks(tick_marks, class_names, fontsize=9)\nplt.ylabel('True Label'); plt.xlabel('Predicted Label')\n\nannotate = (len(class_names) <= 30)\nif annotate:\n    thresh = cm.max() / 2.0 if cm.size > 0 else 0.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], 'd'),\n                     ha=\"center\", va=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nfig_cm.savefig(cm_img_path, dpi=200, bbox_inches=\"tight\")\nif SHOW_PLOTS: plt.show()\nelse: plt.close(fig_cm)\nprint(f\"[Saved] Confusion matrix → {cm_img_path}\")\n\n\nnum_classes = len(class_names)\nif num_classes <= 1:\n    print(\"Skipping ROC: need at least 2 classes.\")\nelse:\n    y_true_bin = label_binarize(all_labels, classes=list(range(num_classes)))\n    present = (y_true_bin.sum(axis=0) > 0)\n\n    fpr, tpr, roc_auc = {}, {}, {}\n    plotted = 0\n    fig_roc = plt.figure(figsize=(14, 10))\n    for i in range(num_classes):\n        if not present[i]:\n            continue\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        plt.plot(fpr[i], tpr[i], linestyle='-', lw=1.5,\n                 label=f\"{class_names[i]} (AUC={roc_auc[i]:.2f})\")\n        plotted += 1\n\n    if plotted == 0:\n        plt.close(fig_roc)\n        print(\"Skipping ROC: none of the classes had positives in y_true.\")\n    else:\n        plt.plot([0,1],[0,1],'k:', lw=2)\n        plt.xlim([0.0,1.0]); plt.ylim([0.0,1.05])\n        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n        plt.title(\"Multi-class ROC (One-vs-Rest)\")\n        plt.legend(loc=\"lower right\", fontsize=9, ncol=2)\n        plt.grid(True); plt.tight_layout()\n        fig_roc.savefig(roc_img_path, dpi=200, bbox_inches=\"tight\")\n        if SHOW_PLOTS: plt.show()\n        else: plt.close(fig_roc)\n        print(f\"[Saved] ROC curves → {roc_img_path}\")","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"id":"94f2de08-5576-4ce0-9159-f415a5bb1037","cell_type":"code","source":"#Part-60:\n\nfrom typing import Optional\nimport random\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\ndef predict_and_plot_pretty(\n    img_path: str,\n    model,\n    transform,\n    class_names,\n    device,\n    test_accuracy: Optional[float] = None,\n    save: bool = True,\n):\n    image = Image.open(img_path).convert(\"RGB\")\n    x = transform(image).unsqueeze(0).to(device)\n    x = x.to(memory_format=torch.channels_last)\n\n    with torch.no_grad():\n        with autocast(enabled=USE_AMP):\n            logits = model(x)\n            probs = F.softmax(logits, dim=1).cpu().numpy().ravel()\n\n    order = np.argsort(probs)[::-1]\n    probs_sorted = probs[order]\n    labels_sorted = [class_names[i] for i in order]\n\n    pred_label = labels_sorted[0]\n    pred_conf = float(probs_sorted[0])\n\n    n = len(class_names)\n    fig_h = max(6, 0.36 * n)\n    fig = plt.figure(figsize=(12, fig_h))\n\n    ax_img = fig.add_subplot(1, 2, 1)\n    ax_img.imshow(image)\n    ax_img.axis(\"off\")\n\n    title = f\"Predicted: {pred_label} (Conf: {pred_conf:.3f})\"\n    if test_accuracy is not None:\n        title += f\"   |   Test Accuracy: {test_accuracy*100:.2f}%\"\n    fig.suptitle(title, y=0.98, fontsize=14)\n\n    ax = fig.add_subplot(1, 2, 2)\n    y = np.arange(n)[::-1]\n    vals = probs_sorted[::-1]\n    labs = labels_sorted[::-1]\n    bars = ax.barh(y, vals)\n    ax.set_yticks(y)\n    ax.set_yticklabels(labs, fontsize=10)\n    ax.set_xlabel(\"Confidence\")\n    ax.set_title(\"Class Probabilities\")\n    ax.set_xlim(0.0, 1.0)\n    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.25)\n    for gx in [0.2, 0.4, 0.6, 0.8]:\n        ax.axvline(gx, color=\"gray\", alpha=0.08, linewidth=1)\n\n    for rect, p in zip(bars, vals):\n        x_end = rect.get_width()\n        y_mid = rect.get_y() + rect.get_height() / 2\n        ax.text(\n            min(x_end + 0.01, 0.98),\n            y_mid,\n            f\"{p:.3f}\",\n            va=\"center\",\n            ha=\"left\" if x_end < 0.95 else \"right\",\n            fontsize=9\n        )\n\n    plt.tight_layout()\n\n    if save:\n        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n        base = os.path.splitext(os.path.basename(img_path))[0]\n        out_path = os.path.join(SAVE_OUTPUT_DIR, f\"prediction_{base}_{ts}.jpg\")\n        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n        print(f\"[Saved] Prediction plot → {out_path}\")\n    plt.show()\n\n    return pred_label, pred_conf\n\n\ndef show_random_images(n=5, save=True):\n    random.seed(None)\n    splits = {\"train\": TRAIN_DIR, \"val\": VAL_DIR, \"test\": TEST_DIR}\n    all_imgs = []\n    for split_name, base in splits.items():\n        if not os.path.isdir(base):\n            continue\n        classes = [c for c in sorted(os.listdir(base)) if os.path.isdir(os.path.join(base, c))]\n        for cls in classes:\n            cdir = os.path.join(base, cls)\n            if not os.path.isdir(cdir):\n                continue\n            for f in os.listdir(cdir):\n                ext = os.path.splitext(f.lower())[1]\n                if ext in {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}:\n                    all_imgs.append((split_name, os.path.join(cdir, f)))\n\n    if not all_imgs:\n        print(\"[WARN] No images found in train/val/test directories.\")\n        return []\n\n    k = min(n, len(all_imgs))\n    sampled = random.sample(all_imgs, k=k)\n\n    results = []\n    for i, (split_name, fp) in enumerate(sampled, start=1):\n        print(f\"[{i}/{k}] Random from {split_name}: {fp}\")\n        pred, conf = predict_and_plot_pretty(\n            fp, model, eval_tf, class_names, device,\n            test_accuracy=(globals().get('test_acc', None)),\n            save=save\n        )\n        results.append((fp, pred, conf))\n    return results\n\n_ = show_random_images(n=5, save=True)","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"id":"e9062d3d-cdb1-4f07-b41b-782679a70d6c","cell_type":"code","source":"#Part-61:\n\nfrom typing import Optional\nimport os, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\ndef predict_and_plot_all(\n    img_path: str,\n    model,\n    transform,\n    class_names,\n    device,\n    test_accuracy: Optional[float] = None,\n    save: bool = True,\n):\n    assert os.path.isfile(img_path), f\"Image not found: {img_path}\"\n    model.eval()\n\n    image = Image.open(img_path).convert(\"RGB\")\n    x = transform(image).unsqueeze(0).to(device)\n    x = x.to(memory_format=torch.channels_last)\n\n    with torch.no_grad():\n        with autocast(enabled=USE_AMP):\n            logits = model(x)\n            probs = F.softmax(logits, dim=1).cpu().numpy().ravel()\n\n    idx_sorted = np.argsort(probs)[::-1]\n    sorted_probs  = probs[idx_sorted]\n    sorted_labels = [class_names[i] for i in idx_sorted]\n\n    pred_idx   = idx_sorted[0]\n    pred_label = class_names[pred_idx]\n    pred_conf  = float(sorted_probs[0])\n\n    n_classes = len(class_names)\n    h = max(6, 0.35 * n_classes)\n    fig = plt.figure(figsize=(12, h))\n\n    ax_img = fig.add_subplot(1, 2, 1)\n    ax_img.imshow(image)\n    ax_img.axis(\"off\")\n\n    title = f\"Predicted: {pred_label} (Conf: {pred_conf:.3f})\"\n    if test_accuracy is not None:\n        title += f\"   |   Test Accuracy: {test_accuracy*100:.2f}%\"\n    fig.suptitle(title, y=0.98, fontsize=14)\n\n    ax_bar = fig.add_subplot(1, 2, 2)\n    y_pos = np.arange(n_classes)[::-1]\n    ax_bar.barh(y_pos, sorted_probs[::-1])\n    ax_bar.set_yticks(y_pos)\n    ax_bar.set_yticklabels(sorted_labels[::-1], fontsize=10)\n    ax_bar.set_xlabel(\"Confidence\")\n    ax_bar.set_title(\"Class Probabilities\")\n    ax_bar.set_xlim(0, 1.0)\n    ax_bar.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n\n    rev_probs = sorted_probs[::-1]\n    for i, p in enumerate(rev_probs):\n        ax_bar.text(\n            min(p + 0.01, 0.98),\n            y_pos[i],\n            f\"{p:.3f}\",\n            va=\"center\",\n            ha=\"left\" if p < 0.95 else \"right\",\n            fontsize=9,\n        )\n\n    plt.tight_layout()\n\n    if save:\n        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n        base = os.path.splitext(os.path.basename(img_path))[0]\n        out_path = os.path.join(SAVE_OUTPUT_DIR, f\"predict_all_{base}_{ts}.jpg\")\n        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n        print(f\"[Saved] Prediction plot → {out_path}\")\n\n    plt.show()\n    return pred_label, pred_conf\n\n\n\nIMG_PATH = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/art_classification_model/dataset/test/pop_art/11.png\"\n\n_ = predict_and_plot_all(\n    IMG_PATH,\n    model,\n    eval_tf,\n    class_names,\n    device,\n    test_accuracy=(globals().get('test_acc', None)),\n    save=True\n)","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"id":"142a173b-4eb7-422f-b8b7-8d637b04451c","cell_type":"code","source":"#Part-62: \n\nimport os\nimport random\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import amp\n\n\nSAVE_OUTPUT_DIR = \"/home/dhrubo/Desktop/MerakiNexus-V2-main/OutPut/AlexNet\"\nos.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n\nSHOW_PLOTS = True           \nNUM_IMAGES = 4              \nUSE_PREDICTED_CLASS = True \n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n\n\ndef _collect_all_images() -> List[Tuple[str, str]]:\n    \"\"\"Return list of (split, filepath) across train/val/test, image files only.\"\"\"\n    splits = {\"train\": TRAIN_DIR, \"val\": VAL_DIR, \"test\": TEST_DIR}\n    items = []\n    for split_name, base in splits.items():\n        if not os.path.isdir(base):\n            continue\n        classes = [c for c in sorted(os.listdir(base)) if os.path.isdir(os.path.join(base, c))]\n        for cls in classes:\n            cdir = os.path.join(base, cls)\n            if not os.path.isdir(cdir):\n                continue\n            for f in os.listdir(cdir):\n                ext = os.path.splitext(f.lower())[1]\n                if ext in IMG_EXTS:\n                    items.append((split_name, os.path.join(cdir, f)))\n    return items\n\ndef _load_image_as_tensor(path: str):\n    \"\"\"Load image file and apply eval_tf -> tensor on device, channels_last.\"\"\"\n    img = Image.open(path).convert(\"RGB\")\n    x = eval_tf(img).unsqueeze(0).to(device)\n    x = x.to(memory_format=torch.channels_last)\n    return img, x\n\n\nclass GradCAMpp:\n    \"\"\"\n    Grad-CAM++ implementation targeting the last Conv2d layer by default.\n    Works for AlexNet (features[10] pre-ReLU conv) and most CNNs.\n    \"\"\"\n    def __init__(self, model: torch.nn.Module, target_layer: torch.nn.Module = None):\n        \n        self.model = getattr(model, \"_orig_mod\", model)\n        self.model.eval()\n\n        self.target_layer = target_layer if target_layer is not None else self._find_last_conv(self.model)\n\n        if self.target_layer is None:\n            raise RuntimeError(\"Could not find a Conv2d layer to attach Grad-CAM++.\")\n\n        self.activations = None  \n       \n        def _forward_hook(module, inp, out):\n            self.activations = out\n\n        self.handle_fwd = self.target_layer.register_forward_hook(_forward_hook)\n\n    def _find_last_conv(self, m: torch.nn.Module):\n        last = None\n        for mod in m.modules():\n            if isinstance(mod, torch.nn.Conv2d):\n                last = mod\n        return last\n\n    @torch.no_grad()\n    def _upsample_cam(self, cam: torch.Tensor, target_size: Tuple[int, int]) -> np.ndarray:\n        # cam: (1,1,h,w)\n        cam_up = F.interpolate(cam, size=target_size, mode=\"bilinear\", align_corners=False)\n        cam_up = cam_up.squeeze().cpu().numpy()\n        cam_up = (cam_up - cam_up.min()) / (cam_up.max() - cam_up.min() + 1e-8)\n        return cam_up\n\n    def generate(self, x: torch.Tensor, class_idx: int = None) -> np.ndarray:\n        \"\"\"\n        x: (1,3,H,W) tensor\n        class_idx: target class; if None, uses predicted argmax.\n        returns: cam heatmap (H,W) normalized to [0,1]\n        \"\"\"\n      \n        x = x.requires_grad_(True)\n        self.model.zero_grad(set_to_none=True)\n\n        \n        with amp.autocast(\"cuda\", enabled=False):\n            logits = self.model(x)  # (1, C)\n            if class_idx is None:\n                class_idx = int(logits.argmax(dim=1).item())\n            score = logits[0, class_idx]\n\n        \n        A = self.activations        \n        if A is None:\n            raise RuntimeError(\"Forward hook did not capture activations. Make sure target layer is correct.\")\n\n        \n        grads = torch.autograd.grad(score, A, retain_graph=True, create_graph=True)[0]  \n\n       \n        grads_power_2 = grads * grads                      \n        grads_power_3 = grads_power_2 * grads              \n    \n        sum_A_grads3 = (A * grads_power_3).sum(dim=(2, 3), keepdim=True)  \n\n        eps = 1e-8\n        alpha_num   = grads_power_2              \n        alpha_denom = 2.0 * grads_power_2 + sum_A_grads3    \n        alpha = alpha_num / (alpha_denom + eps)             \n\n      \n        positive_grads = F.relu(grads)                     \n\n       \n        weights = (alpha * positive_grads).sum(dim=(2, 3), keepdim=True)  \n\n       \n        cam = F.relu((weights * A).sum(dim=1, keepdim=True))  \n\n        \n        H, W = x.shape[-2:]\n        heat = self._upsample_cam(cam, (H, W))  \n        return heat\n\n    def close(self):\n        try:\n            self.handle_fwd.remove()\n        except Exception:\n            pass\n\ndef overlay_heatmap_on_image(img_pil: Image.Image, heat: np.ndarray, alpha: float = 0.35):\n    \"\"\"Return a matplotlib figure with image + heatmap overlay.\"\"\"\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(1, 2, 1)\n    ax.imshow(img_pil)\n    ax.axis(\"off\")\n    ax.set_title(\"Image\")\n\n    ax2 = fig.add_subplot(1, 2, 2)\n    ax2.imshow(img_pil)\n    ax2.imshow(heat, cmap=\"jet\", alpha=alpha)\n    ax2.axis(\"off\")\n    ax2.set_title(\"Grad-CAM++\")\n    plt.tight_layout()\n    return fig\n\n\ndef run_gradcampp_random(num_images=NUM_IMAGES):\n\n    model.eval()\n\n   \n    cam_engine = GradCAMpp(model)\n\n    \n    all_items = _collect_all_images()\n    if not all_items:\n        print(\"[WARN] No images found in dataset.\")\n        cam_engine.close()\n        return []\n\n    k = min(num_images, len(all_items))\n    random.seed()  \n    sampled = random.sample(all_items, k=k)\n\n    outputs = []\n    for i, (split_name, path) in enumerate(sampled, start=1):\n        try:\n            img_pil, x = _load_image_as_tensor(path)\n\n        \n            with torch.inference_mode():\n                with amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                    logits = model(x)\n                    pred_idx = int(logits.argmax(dim=1).item())\n\n            target_class = pred_idx if USE_PREDICTED_CLASS else pred_idx  \n\n       \n            heat = cam_engine.generate(x, class_idx=target_class)\n\n          \n            fig = overlay_heatmap_on_image(img_pil, heat, alpha=0.35)\n            ts = time.strftime(\"%Y%m%d-%H%M%S\")\n            base = os.path.splitext(os.path.basename(path))[0]\n            out_path = os.path.join(SAVE_OUTPUT_DIR, f\"gradcampp_{base}_{ts}.jpg\")\n            fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n            if SHOW_PLOTS:\n                plt.show()\n            else:\n                plt.close(fig)\n\n            print(f\"[{i}/{k}] {split_name} → {path} | class={class_names[target_class]} | saved → {out_path}\")\n            outputs.append((path, class_names[target_class], out_path))\n        except Exception as e:\n            print(f\"[ERROR] Failed on {path}: {e}\")\n\n    cam_engine.close()\n    return outputs\n\n\n_ = run_gradcampp_random()","metadata":{"tags":[]},"outputs":[],"execution_count":null},{"id":"7e9c8ca8-5dc4-433e-8ca1-75cfbd52c23b","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}